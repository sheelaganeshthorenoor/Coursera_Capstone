{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: TwitterAPI in c:\\users\\sheela.ganesh.t\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (2.5.10)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\sheela.ganesh.t\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from TwitterAPI) (1.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sheela.ganesh.t\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from TwitterAPI) (2.18.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sheela.ganesh.t\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests-oauthlib->TwitterAPI) (3.1.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sheela.ganesh.t\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->TwitterAPI) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\sheela.ganesh.t\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->TwitterAPI) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\sheela.ganesh.t\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->TwitterAPI) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sheela.ganesh.t\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages (from requests->TwitterAPI) (2018.4.16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Number of users collected: 0\n",
      "\n",
      " Number of messages collected: 0\n",
      "\n",
      "Number of communities discovered: 1\n",
      "Cluster  0  number of elements  183\n",
      "Number of instances per class 0\n",
      "Tweets \n",
      " The negative tweets are:  85 \n",
      " The positive tweets are:  431 \n",
      " The balanced tweets are:  0\n",
      "One example from each class:\n",
      "negative: {'tweet': \"Avengers End Game made me cry 3 times during the movie and now it's making me cry a 4th time just thinking about it in bed cool\", 'created_at': 'Fri Apr 26 05:51:31 +0000 2019', 'user_name': 'MaddiePolonus', 'user_id': '730074709', 'location': 'Mizzou', 'token': ['avengers', 'end', 'game', 'made', 'me', 'cry', 'times', 'during', 'the', 'movie', 'and', 'now', 'it', 's', 'making', 'me', 'cry', 'a', 'th', 'time', 'just', 'thinking', 'about', 'it', 'in', 'bed', 'cool'], 'pos': 1, 'neg': 2, 'sentiment': 'negative'}\n",
      "\n",
      "\n",
      "positive: {'tweet': \"RT @fineglorysht4: For all the MCU fans out there. Let's stand and listen to our National Anthem. \\n\\n#AvengersEndgame https://t.co/oDPX1OHIIo\", 'created_at': 'Fri Apr 26 05:51:31 +0000 2019', 'user_name': 'CandyAbleNooB', 'user_id': '2207143130', 'location': '방탄소년단♡ARMY', 'token': ['rt', 'for', 'all', 'the', 'mcu', 'fans', 'out', 'there', 'let', 's', 'stand', 'and', 'listen', 'to', 'our', 'national', 'anthem', 'avengersendgame'], 'pos': 0, 'neg': 0, 'sentiment': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "!pip install TwitterAPI\n",
    "\n",
    "\"\"\"\n",
    "Summarize data.\n",
    "\"\"\"\n",
    "import json\n",
    "import networkx as nx\n",
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Reading data from file\n",
    "    \"\"\"\n",
    "    array = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            array.append(json.loads(line))\n",
    "    return array\n",
    "    pass\n",
    "\n",
    "def read_txt(filename):\n",
    "    \"\"\"\n",
    "    Reading data from file\n",
    "    \"\"\"\n",
    "    array = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            array.append(line)\n",
    "    return array\n",
    "    pass\n",
    "\n",
    "\n",
    "def main():\n",
    "\tfile_collect = read_file(\"classify_data.json\")\n",
    "\tfile_friends = read_file(\"classify_friend.json\")\n",
    "\tfile_cluster = read_txt(\"cluster_data.txt\")\n",
    "\tfile_classify = read_file(\"sentiment_data.json\")\n",
    "\tprint(\"\\n Number of users collected:\", len(file_friends))\n",
    "\tprint(\"\\n Number of messages collected:\",len(file_collect))\n",
    "\tprint(\"\\nNumber of communities discovered:\", len(file_cluster))\n",
    "\tfor i in range(0,len(file_cluster)):\n",
    "\t\tprint(\"Cluster \", i , \" number of elements \" , len(file_cluster[i]))\n",
    "\tnegative_tweets = []\n",
    "\tpositive_tweets = []\n",
    "\tbalanced_tweets = []\n",
    "\tfor u in file_classify:\n",
    "\t\tfor i in u:\n",
    "\t\t\tif i['sentiment'] ==\"negative\":\n",
    "\t\t\t\tnegative_tweets.append(i)\n",
    "\t\t\telif i[\"sentiment\"] == \"positive\":\n",
    "\t\t\t\tpositive_tweets.append(i)\n",
    "\tprint(\"Number of instances per class\", len(file_collect))\n",
    "\tprint(\"Tweets \\n The negative tweets are: \",len(negative_tweets), \"\\n The positive tweets are: \", len(positive_tweets), \"\\n The balanced tweets are: \",len(balanced_tweets))\n",
    " \t#print(\"Number of instances per class found \",len(file_collect))\n",
    " \t#print(\" tweets \\n The negative tweets are: \",len(negative_tweets), \"\\n The positive tweets are: \", len(positive_tweets), \"\\n The balanced tweets are: \",len(balanced_tweets))\n",
    "\tprint(\"One example from each class:\")\n",
    "\tprint(\"negative:\", negative_tweets[0])\n",
    "\tprint(\"\\n\\npositive:\", positive_tweets[0])\n",
    "\tpass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "collect.py\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from TwitterAPI import TwitterAPI\n",
    "import time\n",
    "import pickle\n",
    "import signal\n",
    "from threading import Thread, Condition\n",
    "from multiprocessing import Queue\n",
    "import sys\n",
    "import json\n",
    "import threading\n",
    "import numpy as np\n",
    "\n",
    "queueSize = 500\n",
    "queue = Queue(queueSize)\n",
    "twitter_rate_limit = Condition()\n",
    "#Filename\n",
    "CLASSIFY_FILE_NAME= \"classify_data.json\"\n",
    "CLASSIFY_FRIEND_NAME= \"classify_friend.json\"\n",
    "\n",
    "# Service Tokens\n",
    "consumer_key = 'ftwgjnC4SR8U4lEPNHqNKiNCV'\n",
    "consumer_secret = 'DiIXS7Z0NkrYgtes9zbD666AwhJiC9AM4lBTuv8aMrKOAb7jBC'\n",
    "access_token = '596398510-mYstiQ7CekWvoqXsDKdi7CxxaRgiGlTsgsnigT5z'\n",
    "access_token_secret = 'i5a2c5379p1UCCYbJfLnkEdIycDRuR9YvjLvzHCxSA410'\n",
    "\n",
    "\n",
    "class CreateData(Thread):\n",
    "    def __init__(self, run_event, service, keywords,file):\n",
    "        self.run_event = run_event\n",
    "        self.service = service\n",
    "        self.comments = self.robust_request(self.service,'statuses/filter', {'language': 'en', 'track': keywords})\n",
    "        self.file = file\n",
    "        Thread.__init__(self)\n",
    "        \n",
    "    def run(self):\n",
    "        global queue\n",
    "        for comment in self.comments:\n",
    "            if not self.run_event.is_set():\n",
    "                break\n",
    "            try:\n",
    "                print(comment['text'])\n",
    "            except:\n",
    "                continue\n",
    "            queue.put(comment)\n",
    "            data = {}\n",
    "            data[\"tweet\"] = comment[\"text\"]\n",
    "            data[\"created_at\"] = comment[\"created_at\"]\n",
    "            data[\"user_name\"] = comment['user'][\"screen_name\"]\n",
    "            data[\"user_id\"] = str(comment['user'][\"id\"])\n",
    "            data[\"location\"] = comment['user'][\"location\"]\n",
    "            json.dump(data, self.file)\n",
    "            self.file.write(\"\\n\")\n",
    "            \n",
    "    def robust_request(self, twitter, resource, params, max_tries=5):\n",
    "        for i in range(max_tries):\n",
    "            request = twitter.request(resource, params)\n",
    "            if request.status_code == 200:\n",
    "                return request\n",
    "            else:\n",
    "                print('Got error %s \\nsleeping for 15 minutes.' % request.text)\n",
    "                sys.stderr.flush()\n",
    "                twitter_rate_limit.acquire()\n",
    "                twitter_rate_limit.wait(61 * 15)\n",
    "                twitter_rate_limit.release()\n",
    "                if not self.run_event.is_set():\n",
    "                    break\n",
    "\n",
    "class CollectFriendData(Thread):\n",
    "    def __init__(self, run_event, service, keywords, file):\n",
    "        self.run_event = run_event\n",
    "        self.service = service\n",
    "        self.users_file = file\n",
    "        Thread.__init__(self)\n",
    "        \n",
    "    def run(self):\n",
    "        global queue\n",
    "        while self.run_event.is_set():\n",
    "            comment = queue.get()\n",
    "            user_id = comment['user']['id']\n",
    "            request = self.robust_request(self.service, \"friends/ids\", {'user_id': user_id})\n",
    "            if request is None:\n",
    "                continue\n",
    "            data = {}\n",
    "            data[\"user_id\"] = str(comment['user'][\"id\"])\n",
    "            data[\"user_name\"] = comment['user'][\"screen_name\"]\n",
    "            data[\"friends\"] = list(request)\n",
    "            json.dump(data, self.users_file)\n",
    "            self.users_file.write(\"\\n\")      \n",
    "            \n",
    "            \n",
    "    def robust_request(self, twitter, resource, params, max_tries=5):\n",
    "        for i in range(max_tries):\n",
    "            request = twitter.request(resource, params)\n",
    "            if request.status_code == 200:\n",
    "                return request\n",
    "            else:\n",
    "                print('Got error %s \\nsleeping for 15 minutes.' % request.text)\n",
    "                sys.stderr.flush()\n",
    "                twitter_rate_limit.acquire()\n",
    "                twitter_rate_limit.wait(61 * 15)\n",
    "                twitter_rate_limit.release()\n",
    "                if not self.run_event.is_set():\n",
    "                    break\n",
    "\n",
    "def yes_or_no(question):\n",
    "    reply = str(input(question+' (y/n): ')).lower().strip()\n",
    "    if reply[0] == 'y':\n",
    "        x = input(\"Please keywords you want to have use for analysis in one line using commas\\n (Recommanded related keywords for better search) \\n\").split(',')\n",
    "        keywords = np.asarray(x)\n",
    "        return keywords\n",
    "    if reply[0] == 'n':\n",
    "        return True \n",
    "    else:\n",
    "        return yes_or_no(\"Uhhhh... please enter y or n \")\n",
    "pass\n",
    "\n",
    "def main():\n",
    "    run_event = threading.Event()\n",
    "    run_event.set()\n",
    "    service = TwitterAPI(consumer_key, consumer_secret, access_token, access_token_secret)\n",
    "    file1 = open(CLASSIFY_FILE_NAME, \"w\")\n",
    "    file2 = open(CLASSIFY_FRIEND_NAME,\"w\")\n",
    "    keywords = [\"Avengers End Game\", \"Avengers\", \"End Game\", \"MCU\" , \"Marvel\", \"Marvel Cinematic Universe\", \"Iron Man\",\"Thanos\", \"Hulk\"]\n",
    "    answer = yes_or_no(\"This Analysis shows results for 'Avengers:EndGame' movie, Do you want to try something else?\")\n",
    "    if(isinstance(answer,np.ndarray)):\n",
    "        keywords = answer\n",
    "    collect_comments = CreateData(run_event, service, keywords,file1)\n",
    "    collect_friends = CollectFriendData(run_event, service, keywords, file2)\n",
    "    try:\n",
    "        collect_comments.start()\n",
    "        collect_friends.start()\n",
    "    except:\n",
    "        run_event.clear()\n",
    "        twitter_rate_limit.acquire()\n",
    "        twitter_rate_limit.notify()\n",
    "        twitter_rate_limit.release()\n",
    "        collect_comments.join()\n",
    "        collect_friends.join()\n",
    "        file1.close()\n",
    "        file2.close()\n",
    "        sys.exit()\n",
    "        quit()\n",
    "    try:    \n",
    "        while queue.qsize() < queueSize:\n",
    "            time.sleep(.1)\n",
    "        raise Exception(\"Exit\")\n",
    "    except Exception:\n",
    "        print(\"Data Collected! Exiting\")\n",
    "        run_event.clear()\n",
    "        twitter_rate_limit.acquire()\n",
    "        twitter_rate_limit.notify()\n",
    "        twitter_rate_limit.release()\n",
    "        collect_comments.join()\n",
    "        collect_friends.join()\n",
    "        file1.close()\n",
    "        file2.close()\n",
    "        sys.exit()\n",
    "        quit()\n",
    "       \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "\"\"\"\n",
    "Classify data.\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "\n",
    "CLASSIFY_FILE_NAME= \"classify_data.json\"\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Reading data from file\n",
    "    \"\"\"\n",
    "    array = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            array.append(json.loads(line))\n",
    "    return array\n",
    "    pass\n",
    "\n",
    "def get_tokens(tweets):\n",
    "    \n",
    "    tokens = []\n",
    "    for tweet in tweets:\n",
    "        tweet[\"token\"] = []\n",
    "        text = tweet['tweet'].lower()\n",
    "        text = re.sub('@\\S+', ' ', text)  # Remove mentions.\n",
    "        text = re.sub('http\\S+', ' ', text)  # Remove urls.\n",
    "        token = re.findall('[A-Za-z]+', text)\n",
    "        tokens.append(token) # Retain words.\n",
    "        #print(token)\n",
    "        tweet[\"token\"] = token\n",
    "    return tokens\n",
    "    pass\n",
    "\n",
    "def purne_token(tweets, wordCount):\n",
    "    word_counts = Counter()\n",
    "    for tweet in tweets:\n",
    "        word_counts.update(tweet[\"token\"])\n",
    "    # Retain in vocabulary words occurring more than wordCount.\n",
    "    vocab = set([w for w, c in word_counts.items() if c > wordCount])\n",
    "    newtoks = []\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        newtok = [token for token in tweet[\"token\"] if token in vocab]\n",
    "        if len(newtok) > 0:\n",
    "            newtoks.append(newtok)\n",
    "           \n",
    "    return newtoks\n",
    "    pass\n",
    "\n",
    "def download_afinn():\n",
    "    url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
    "    zipfile = ZipFile(BytesIO(url.read()))\n",
    "    afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
    "    afinn = dict()\n",
    "\n",
    "    for line in afinn_file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            afinn[parts[0].decode(\"utf-8\")] = int(parts[1])\n",
    "    print(\"read %d AFINN terms.\" %(len(afinn)))\n",
    "    return afinn\n",
    "\n",
    "def afinn_posneg(terms, afinn, verbose=False):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "            if verbose:\n",
    "                print('\\t%s=%d' % (t, afinn[t]))\n",
    "            if afinn[t] > 0:\n",
    "                pos += afinn[t]\n",
    "            else:\n",
    "                neg += -1 * afinn[t]\n",
    "    return pos, neg\n",
    "\n",
    "def afinn_sentiment(tweets, afinn):\n",
    "    for u in tweets:\n",
    "            u['pos'], u['neg'] = afinn_posneg(u['token'], afinn) \n",
    "            sentiment = \"balanced\"\n",
    "            if u['pos'] < u['neg']:\n",
    "                sentiment = \"negative\"\n",
    "            else:\n",
    "                sentiment = \"positive\"\n",
    "            u['sentiment'] = sentiment\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    tweets = read_file(CLASSIFY_FILE_NAME)\n",
    "    print(\"Tweets has been read!\")\n",
    "    tokens = get_tokens(tweets)\n",
    "    purned_tokens = purne_token(tweets,2)\n",
    "    afinn = download_afinn()\n",
    "    afinn_sentiment(tweets, afinn)\n",
    "    negative_tweets = [u for u in tweets if u['sentiment'] == \"negative\"]\n",
    "    positive_tweets = [u for u in tweets if u['sentiment'] == \"positive\"]\n",
    "    print(\"In total \",len(tweets),\" tweets \\n The negative tweets are: \",len(negative_tweets), \"\\n The positive tweets are: \",len(positive_tweets))\n",
    "    file = open(\"sentiment_data.json\",\"w\")  \n",
    "    json.dump(tweets,file)\n",
    "    file.close()\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()    \n",
    "\"\"\"\n",
    "Cluster data.\n",
    "\"\"\"\n",
    "import json\n",
    "from collections import Counter, defaultdict, deque\n",
    "import copy\n",
    "from itertools import combinations\n",
    "import math\n",
    "import networkx as nx\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import community\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import community\n",
    "from sklearn.cluster import spectral_clustering\n",
    "import scipy\n",
    "\n",
    "CLASSIFY_FILE_NAME= \"classify_friend.json\"\n",
    "\n",
    "def read_file(filename):\n",
    "    \"\"\"\n",
    "    Reading data from file\n",
    "    \"\"\"\n",
    "    array = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            array.append(json.loads(line))\n",
    "    return array\n",
    "    pass\n",
    "        \n",
    "def build_graph(users):\n",
    "    G = nx.Graph()\n",
    "    for u in users:\n",
    "        G.add_edges_from([str(u['user_id']), str(fid)] for fid in u['friends'])\n",
    "    return G\n",
    "\n",
    "def draw_network(graph, users, filename):\n",
    "    graph_labels=dict()\n",
    "    for u in users:\n",
    "        graph_labels[str(u['user_id'])]=u['user_name']\n",
    "    fig = plt.figure()\n",
    "    plt.axis('off')\n",
    "    nx.draw_networkx(graph,pos=nx.spring_layout(graph),with_labels = True, labels=graph_labels, node_size = 70, width = 0.2)\n",
    "    fig.savefig(filename, format='png')\n",
    "    plt.draw() \n",
    "    pass\n",
    "\n",
    "def bfs(graph, root):\n",
    "    node2distances = defaultdict(int)\n",
    "    node2num_paths = defaultdict(int)\n",
    "    node2parents = defaultdict(list)\n",
    "\n",
    "    nodequeue = deque([root])\n",
    "    visited = set()\n",
    "    visited.add(root)\n",
    "\n",
    "    node2distances[root] = 0\n",
    "    node2num_paths[root] = 1\n",
    "    depth = 0\n",
    "    parent = root\n",
    "    while (len(nodequeue) > 0):\n",
    "        node = \"\".join(nodequeue.popleft())\n",
    "        depth = node2distances[node] + 1\n",
    "        for n in graph.neighbors(node):\n",
    "            if (n not in visited):\n",
    "                nodequeue.append ([n])\n",
    "                node2distances[n] = depth\n",
    "                node2parents[n].append(node)\n",
    "                node2num_paths[n] = 1\n",
    "                visited.add(n)\n",
    "            elif (node2distances[n] == depth):\n",
    "                node2num_paths[n] += 1\n",
    "                node2parents[n].append(node)\n",
    "              \n",
    "    return dict(sorted(node2distances.items())), dict(sorted(node2num_paths.items())), dict(sorted((_node, sorted(_parents)) for _node, _parents in node2parents.items()))\n",
    "    pass\n",
    "\n",
    "def bottom_up(root, node2distances, node2num_paths, node2parents):\n",
    "    result = dict()\n",
    "    cnt = Counter()\n",
    "    for n in sorted(node2distances, key=node2distances.get, reverse=True):\n",
    "        if n != root:\n",
    "            if node2num_paths[n] > 1:\n",
    "                for a in node2parents[n]:\n",
    "                    finalTuple = tuple(sorted([n, a]))\n",
    "                    result[finalTuple]=(float)((cnt[n]+1)/len(node2parents[n]))\n",
    "                    cnt[a]=(float)(cnt[a]+result[finalTuple])\n",
    "            else:\n",
    "                l = tuple(sorted([n,node2parents[n][0]]))\n",
    "                result[l] = float(cnt[n] + 1) \n",
    "                cnt[node2parents[n][0]] = (float)(cnt[node2parents[n][0]]+result[l])\n",
    "    \n",
    "    return result\n",
    "    pass\n",
    "\n",
    "def approximate_betweenness(graph):\n",
    "    betweenness = dict()\n",
    "    for n in graph.nodes():\n",
    "        node2distances, node2num_paths, node2parents = bfs(graph, n)\n",
    "        botup = bottom_up(n, node2distances, node2num_paths, node2parents)\n",
    "        for b in botup:\n",
    "            if b in betweenness:\n",
    "                betweenness[b] = betweenness[b] + botup[b]\n",
    "            else:\n",
    "                betweenness[b] = botup[b]\n",
    "    for a,b in betweenness.items():\n",
    "        betweenness[a]=b/2\n",
    "\n",
    "    return betweenness\n",
    "    pass\n",
    "\n",
    "def partition_girvan_newman(graph):\n",
    "    index=0\n",
    "    gcopy=graph.copy()\n",
    "    apxbtw = approximate_betweenness(gcopy)    \n",
    "    result=sorted(apxbtw.items(),key=lambda x: (-x[1], x[0]))\n",
    "    compnent =[c for c in nx.connected_component_subgraphs(gcopy)]\n",
    "    while (True):\n",
    "        gcopy.remove_edge(*(result[index][0]))\n",
    "        compnent = [c for c in nx.connected_component_subgraphs(gcopy)]\n",
    "        if len(compnent) > 1:\n",
    "            break\n",
    "        elif len(compnent) == 1:\n",
    "            index += 1\n",
    "    return compnent\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    users = read_file(CLASSIFY_FILE_NAME)\n",
    "    print(\"File has been read!\")\n",
    "    graph = build_graph(users)\n",
    "    print(\"Graph in progress..... This might take a while...\")\n",
    "    draw_network(graph,users,\"cluster_img.png\")\n",
    "    print(\"Graph Stored in cluster_img.png\")\n",
    "    clusters = partition_girvan_newman(graph)\n",
    "    print('%d clusters' % len(clusters))\n",
    "    print('first partition:');\n",
    "    for i in range(0,len(clusters)):\n",
    "        print(\"cluster \",i,\" has  \",clusters[i].order(),\"  nodes\")      \n",
    "    file = open(\"cluster_data.txt\",\"w\")  \n",
    "    file.write(str(clusters))\n",
    "    file.close()\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install TwitterAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
